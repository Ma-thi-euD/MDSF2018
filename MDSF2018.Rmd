---
title: "Tutoriel R - MDSF 2018"
output:
  html_document:
    df_print: paged
---


Ce tutoriel a pour but de guider les personnes souhaitant utiliser `R` pour participer au challenge.

Il comporte cinq √©tapes :

1. Import des donnees
2. Analyse descriptive
3. Preparation des donnees
4. Creation d'un modele 
5. Calcul des predictions et soumission

# Import des donn√©es

Avant de rentrer dans le vif du sujet, installons les packages necessaires pour ce tutoriel :
```{r message=FALSE, warning=FALSE}
if (require(readr) == F){install.packages("readr")} ; library(readr)
if (require(dplyr) == F){install.packages("dplyr")} ; library(dplyr)
if (require(MLmetrics) == F){install.packages("MLmetrics")} ; library(MLmetrics)
if (require(randomForest) == F){install.packages("randomForest")} ; library(randomForest)
```

Le datasets sont sous format .csv. Vous pouvez les importer dans l'espace de travail R avec le code ci-dessous. Pensez √† changer le chemin du dossier contenant les data.
```{r message=FALSE, warning=TRUE}
# Penser √† remplacer par le bon chemin o√π sont les donn√©es sur votre poste.
x_train <- read_csv("X_train.csv")
y_train <- read_csv("y_train.csv")
x_test  <- read_csv("X_test.csv")

# Retire les lignes qui ont un probleme de parsing dans x_train
if(length(problems(x_train)$row) > 0){
 lignes_problematiques <- unique(problems(x_train)$row)
 x_train <- x_train[-lignes_problematiques,] 
}
```

On est maintenant pr√™t √† attaquer les choses serieuses ! :-)


## Structure des datasets

Le dataset `x_train` d√©crit les caracteristiques de 8779 objets vendus sur le site Emmaus. Le dataset `y_train` d√©crit le d√©lai de vente de ces m√™mes objets. Ce sont ces datasets que nous allons utiliser pour cr√©er un modele. Chaque objet est decrit par une observation de 30 variables hors index. Ces variables sont d√©crites dans le fichier de description pr√©sent dans la cl√© usb.

Le dataset `test` comporte les caracteristiques des 2927 objets dont il faut predire le delai de vente. A la difference du train, le d√©lai de vente n'est bien sur pas renseign√© et une colonne id a ete rajout√© pour identifier les pr√©dictions pendant l'√©tape de soumission. 

## Distribution des donnees

Jetons maintenant un coup d'oeil √† la distribution des donn2es

```{r}
prop.table(table(y_train$delai_vente))
```
Le jeu de donn√©es est tr√®s √©quilibr√©e, chacune des 3 classes a une fr√©quence proche d'1/3.

# Pr√©paration des donn√©es

Pour faciliter la pr√©paration de donn√©es, il est conseill√© de concatener les datasets `train` et `test` pour n'avoir √† modifier qu'un dataset, quitte √† les separer de nouveau par la suite.

```{r message=FALSE, warning=FALSE}
# Cree une variable partition pour pouvoir separer les datasets apres la preparation des donnees
train <- inner_join(x_train, y_train, by ='id')
train$partition <- 'train' 
x_test$partition <- 'test'

full <- bind_rows(train, x_test) # Combine train et test pour avoir un seul dataset √† manipuler
```


## Imputation des valeurs manquantes
```{r}
# Impute les valeurs manquantes par une valeur arbitraire (-999) pour les variables poids, largeurs et longueurs d'images.
# full$poids[is.na(full$poids)] <- -999
# full$largeur_image[is.na(full$largeur_image)] <- -999
# full$longueur_image[is.na(full$longueur_image)] <- -999

# StatKalli : remplacement par la mediane ou la moyenne
summary(full$poids)
full$poids[is.na(full$poids)] <- 500 #median
summary(full$largeur_image)
full$largeur_image[is.na(full$largeur_image)] <- 1800.7 #moyenne
summary(full$longueur_image)
full$longueur_image[is.na(full$longueur_image)] <- 1805 #moyenne

#Impute les valeurs manquantes de la variable cat√©gorie par la modalit√© la plus fr√©quente).
full$categorie[is.na(full$categorie)] <- 'mode'

#Change le type de la variable "categorie" en factor.
full$categorie <- factor(full$categorie) 

# StatKalli : ajout de la variable etat, remplacement des valeurs NA
full$etat[is.na(full$etat)] <- 'bon Ètat'
full$etat <- factor(full$etat) 

# StatKalli : traitement de la variable sous_categorie_1/2/3 : combinaison comme pour le MDSF2016
if (require(Hmisc) == F){install.packages("Hmisc")} ; library(Hmisc)
full$sous_categorie_1_combined      = combine.levels(full$sous_categorie_1,      minlev=.01)
full$sous_categorie_1_combined[is.na(full$sous_categorie_1_combined)] <- 'OTHER'
full$sous_categorie_1_combined <- factor(full$sous_categorie_1_combined) 

full$sous_categorie_2_combined      = combine.levels(full$sous_categorie_2,      minlev=.01)
full$sous_categorie_2_combined[is.na(full$sous_categorie_2_combined)] <- 'OTHER'
full$sous_categorie_2_combined <- factor(full$sous_categorie_2_combined) 

full$sous_categorie_3_combined      = combine.levels(full$sous_categorie_3,      minlev=.01)
full$sous_categorie_3_combined[is.na(full$sous_categorie_3_combined)] <- 'OTHER'
full$sous_categorie_3_combined <- factor(full$sous_categorie_3_combined) 


full$sous_categorie_4_combined      = combine.levels(full$sous_categorie_4,      minlev=.01)
full$sous_categorie_4_combined[is.na(full$sous_categorie_4_combined)] <- 'OTHER'
full$sous_categorie_4_combined <- factor(full$sous_categorie_4_combined) 
# StatKalli : trop peu renseignÈe, sous_categorie_4 ne sera pas retenue

# StatKalli : nom_magasin
full$nom_magasin[is.na(full$nom_magasin)]
full$nom_magasin <- as.factor(full$nom_magasin)

# StatKalli : crÈation de 3 variables : vetement, chaussures et wifi par rapport aux variables taille, pointure et wifi
# Il semblerait que l'on puisse dÈfinir le type du produit
full$vetement <- "0"
full[is.na(full$taille) == F,"vetement"] <- "1"
full$vetement <- as.factor(full$vetement)

full$chaussures <- "0"
full[is.na(full$pointure) == F,"chaussures"] <- "1"
full$chaussures <- as.factor(full$chaussures)

full$elec <- "0"
full[is.na(full$wifi) == F,"elec"] <- "1"
full[full$wifi == "False","elec"] <- "1"
full$elec <- as.factor(full$elec)
```


## Split train / test
Nous avons plus haut fusionn√© `train` et `test` dans `full` pour gagner du temps lors de la cr√©ation de variables (un seul dataset √† modifier).

On peut maintenant les s√©parer de nouveau en utilisant la variable `partition`.

```{r}
train <- full[full$partition == 'train',]
test  <- full[full$partition == 'test',]
```

# Cr√©ation d'un premier mod√®le

Il est maintenant temps de creer un premier mod√®le. Dans ce tutoriel nous allons construire une [For√™t Al√©atoire](https://fr.wikipedia.org/wiki/For%C3%AAt_d%27arbres_d%C3%A9cisionnels).

Pour ce faire nous utilisons six variables : categorie, poids, prix, nb_images, largeur_image et longueur_image

Pour √©viter le surapprentissage et estimer de mani√®re fiable les performances de notre mod√®le nous allons utiliser le crit√®re de [validation crois√©e](https://fr.wikipedia.org/wiki/Validation_crois%C3%A9e), methode k-fold.

Pour ce faire nous allons d√©couper l'√©chantillon de train en 5. A chaque it√©ration 4 fold sur 5 (80% du train) serviront √† entrainer les mod√®les, le cinquieme fold (20% du train) sera utilis√© pour valider les performances.

```{r}
K <- 5 # on partitionne l'echantillon de train en 5
set.seed(123) # 
train$cv_id <- sample(1:K, nrow(train), replace = TRUE)

logloss_vector <- c()

for(i in 1:K){
  train_cv <- train[train$cv_id != i, ]
  test_cv  <- train[train$cv_id == i, ]
  
  rf <- randomForest(data = train_cv,
                  as.factor(delai_vente) ~ categorie + poids + prix + nb_images + largeur_image + longueur_image
                     + etat
                     + sous_categorie_1_combined
                     + sous_categorie_2_combined
                     + sous_categorie_3_combined
                     + nom_magasin
                     + nb_caract
                     + nb_mot
                     + vetement
                     + chaussures
                     + elec
                  , ntree = 200)
  
  pred <- predict(rf, test_cv, type = "prob")
  logloss <- MultiLogLoss(y_true = test_cv$delai_vente, 
                         y_pred = pred)

  
  print(logloss)
  logloss_vector <- append(logloss_vector, logloss)
  
}
print(paste0('Moyenne score CV : ', mean(logloss_vector)))
```

Avec ces 5 variables et un nombre d'arbre `ntree`= 10 la logloss en CV est d'environ 3. 

# Calcul des pr√©dictions et soumission
Maintenant que nous avons cr√©√© un modele predictif, il est temps de predire les d√©lais de ventes des objets de l'echantillon de test :
```{r}
delai_test <- predict(rf, test, type = "prob") # Predit les d√©lais de ventes des objets de l'echantillon de test
soumission <- data.frame(id = test$id , delai_test) # Cree un data.frame au bon format pour la soumission
write.table(soumission, file= 'my_prediction.csv', sep= ',', row.names = F) # Sauvegarde la soumission
```

Vous √™tes maintenant pr√™t √† faire votre premiere soumission en uploadant le fichier soumission.csv sur https://qscore.meilleurdatascientistdefrance.com/

## Aller plus loin

Vous pouvez maintenant essayer d'am√©liorer ce premier mod√®le. Pour vous y aider plusieurs indices seront d√©voil√©s au micro pendant le challenge.

**Rappel :** vous n'avez le droit qu'√† 5 soumissions sur la plateforme, utiliser donc la validation crois√©e pour √©valuer vos exp√©rimentations pour ne soumettre que les plus prometteuses.

**Conseil :** une fois que vous avez identifi√© un mod√®le comme prometteur selon son score de validation crois√©e, r√©entrainait le sur l'int√©gralit√© (100%) de la partition de `train`.

Bonne chance pour le challenge et que le meilleuR gagne ! :-)